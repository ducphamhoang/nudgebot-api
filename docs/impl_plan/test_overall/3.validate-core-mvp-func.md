I have created the following plan after thorough exploration and analysis of the codebase. Follow the below plan verbatim. Trust the files and references. Do not re-verify what's written in the plan. Explore only when absolutely necessary. First implement all the proposed file changes and then I'll review all the changes together at the end.

### Observations

The codebase has solid unit test coverage for individual services but lacks comprehensive integration testing for critical MVP flows. Key findings:

**Strengths**: Good unit test patterns, real PostgreSQL integration via testcontainers, comprehensive event handling tests, robust error handling in service layers.

**Critical Gaps**: No end-to-end scheduler/reminder/nudge flow testing, missing HTTP-level command integration tests, no database migration failure scenarios, limited real service integration beyond database.

**Priority Areas**: Scheduler timing flows (highest priority per PRD), command processing via HTTP webhooks, database migration reliability, and cross-service event validation.

### Approach

I analyzed the MVP PRD requirements and conducted a comprehensive audit of the existing test suite across all five critical areas. The analysis revealed significant gaps in test coverage, particularly in scheduler/reminder flows, command processing integration, database migration scenarios, and event-driven architecture validation.

The approach focuses on creating essential tests that fill the most critical gaps identified through cross-referencing the PRD requirements with current test coverage. Priority is given to MVP-critical flows like the complete scheduler→reminder→nudge cycle, HTTP-level command processing, and database error scenarios that could impact production reliability.

### Reasoning

I systematically analyzed the MVP PRD requirements document to extract all functional requirements organized by the five critical areas. I then audited the existing test suite across all test files to understand current coverage depth and identify gaps. I examined the scheduler and reminder flow implementation to understand timing mechanisms and business logic. I investigated the event-driven architecture to understand cross-service communication patterns. I analyzed database operations and migration systems to identify error scenarios. Finally, I examined command processing flows to understand HTTP-level integration gaps.

## Mermaid Diagram

sequenceDiagram
    participant PRD as MVP PRD Requirements
    participant Matrix as Test Coverage Matrix
    participant Tests as Essential Test Suite
    participant CI as Build System
    
    PRD->>Matrix: Extract functional requirements
    Matrix->>Matrix: Map to 5 critical areas
    Matrix->>Tests: Identify coverage gaps
    
    Note over Tests: HIGH PRIORITY
    Tests->>Tests: Scheduler/Reminder/Nudge Flow
    Tests->>Tests: Command Processing HTTP Integration
    Tests->>Tests: Database Migration Failures
    
    Note over Tests: MEDIUM PRIORITY  
    Tests->>Tests: LLM Provider Error Scenarios
    Tests->>Tests: Event Bus Failure Handling
    
    Note over Tests: LOW PRIORITY
    Tests->>Tests: Concurrency & Race Conditions
    
    Tests->>CI: Integration with make targets
    CI->>CI: test-unit, test-integration, test-all
    
    Note over Matrix,CI: Continuous Validation
    Tests-->>Matrix: Update coverage status
    Matrix-->>PRD: Ensure requirement traceability

## Proposed File Changes

### docs\test_coverage_matrix.md(NEW)

References: 

- docs\mvp_prd.md

Create a comprehensive test coverage matrix that maps MVP PRD requirements to existing and planned tests. The matrix will include:

**Structure**:
- Five main sections corresponding to critical areas: Telegram Webhook Handling, LLM Integration, Task Management, Database Operations, Event-Driven Architecture
- For each requirement: User Story reference (US-01 through US-10), functional description, current test coverage status, test files covering it, coverage depth (happy path/edge cases/error scenarios), and gap identification

**Content**:
- **Telegram Webhook Handling**: Map US-01,02,04,05,07,08,09,10 to webhook processing, commands, inline buttons with current coverage in `api/handlers/webhook_test.go` and gaps in HTTP-level command integration
- **LLM Integration**: Map US-02,03,10 to natural language parsing with current coverage in `internal/llm/service_test.go` and gaps in real provider error scenarios
- **Task Management**: Map US-02,04,05,06,07,08,09 to CRUD operations with current coverage in `internal/nudge/service_test.go` and gaps in status transition effects
- **Database Operations**: Map all persistence requirements with current coverage in integration tests and gaps in migration failures
- **Event-Driven Architecture**: Map scheduler and reminder requirements with current coverage in scheduler tests and gaps in end-to-end timing flows

**Gap Markers**: Use ✓ (covered), ⚠ (partial), ✗ (missing) to clearly identify testing priorities

### integration\scheduler_reminder_flow_test.go(NEW)

References: 

- integration_test_helpers.go
- integration\message_flow_test.go
- internal\scheduler\scheduler.go
- internal\nudge\service.go

Create comprehensive integration test for the complete scheduler→reminder→nudge flow, addressing the highest priority gap identified in the analysis.

**Test Setup**:
- Use existing `SetupTestDatabase` helper from `integration_test_helpers.go` for real PostgreSQL via testcontainers
- Create synchronous MockEventBus to ensure deterministic event ordering
- Initialize real scheduler, nudge service, and chatbot service with stub providers
- Implement clock abstraction interface to control timing without actual delays

**Test Scenarios**:
1. **TestSchedulerReminderFlow_DueTaskReminder**: Create task with due_date = now-2min, trigger scheduler tick, verify ReminderDue event published, verify chatbot receives and processes reminder
2. **TestSchedulerReminderFlow_NudgeAfterOverdue**: Create task overdue by >2h, trigger scheduler, verify nudge creation and delivery
3. **TestSchedulerReminderFlow_UserActionAfterReminder**: Send reminder, simulate user clicking 'Done' button, verify task completion and reminder cancellation
4. **TestSchedulerReminderFlow_SnoozeAndReschedule**: Send reminder, simulate snooze action, verify new reminder scheduled at correct time
5. **TestSchedulerReminderFlow_MultipleTasksAndUsers**: Test scheduler handling multiple due tasks for different users simultaneously

**Verification Points**:
- Database state changes (task status, reminder records)
- Event publication and consumption
- Telegram provider message sending (via stub)
- Scheduler metrics and error handling

Reference existing patterns from `integration/message_flow_test.go` for testcontainer setup and event verification.

### integration\command_flow_http_test.go(NEW)

References: 

- api\handlers\webhook.go
- integration\message_flow_test.go
- internal\chatbot\command_processor.go

Create integration tests for end-to-end command processing through HTTP webhook interface, addressing critical gaps in Telegram webhook handling.

**Test Infrastructure**:
- Use Gin test mode with real webhook handler from `api/handlers/webhook.go`
- Set up real database via testcontainers for task persistence
- Use stub providers for Telegram and LLM to avoid external dependencies
- Create helper functions to generate authentic Telegram webhook payloads for commands

**Command Test Scenarios**:
1. **TestCommandFlow_StartCommand**: POST `/start` command webhook, verify welcome message response and user session creation
2. **TestCommandFlow_ListCommand**: POST `/list` command webhook, verify TaskListRequested event, TaskListResponse event, and formatted task list message
3. **TestCommandFlow_DoneCommand**: Create task in DB, POST `/done 1` command webhook, verify task completion and confirmation message
4. **TestCommandFlow_DeleteCommand**: Create task in DB, POST `/delete 1` command webhook, verify task deletion and confirmation
5. **TestCommandFlow_InvalidCommand**: POST invalid command, verify graceful error handling and helpful response
6. **TestCommandFlow_CommandWithInvalidArgs**: POST `/done invalid`, verify argument validation and error message

**Verification Strategy**:
- Assert HTTP 200 response (Telegram requirement)
- Verify database state changes
- Check event publication via MockEventBus
- Validate response message content via stub Telegram provider
- Ensure proper error handling and user feedback

Use existing `createTestTelegramWebhook` pattern from `integration/message_flow_test.go` but extend for command payloads.

### integration\callback_query_flow_test.go(NEW)

References: 

- internal\chatbot\keyboard_builder.go
- internal\chatbot\command_processor.go
- api\handlers\webhook.go

Create integration tests for inline button callback query handling through HTTP webhook, completing Telegram webhook handling coverage.

**Test Setup**:
- Real database and webhook handler setup similar to command flow tests
- Helper functions to create Telegram callback query webhook payloads
- Stub providers for external services
- Pre-create tasks and reminders in database for callback testing

**Callback Query Test Scenarios**:
1. **TestCallbackFlow_TaskDoneButton**: Create task with reminder, POST callback query for 'Done' button, verify task completion and reminder cancellation
2. **TestCallbackFlow_TaskDeleteButton**: Create task, POST callback query for 'Delete' button, verify task deletion
3. **TestCallbackFlow_SnoozeButton**: Create overdue task, POST callback query for 'Snooze' options (15min, 1hour, tomorrow), verify reminder rescheduling
4. **TestCallbackFlow_TaskListNavigation**: Create multiple tasks, POST callback queries for list pagination buttons, verify correct task display
5. **TestCallbackFlow_InvalidCallbackData**: POST callback with malformed or expired data, verify graceful error handling
6. **TestCallbackFlow_CallbackForNonExistentTask**: POST callback for deleted/non-existent task, verify appropriate error response

**Callback Data Testing**:
- Verify JSON encoding/decoding of callback data via `KeyboardBuilder`
- Test callback data size limits and fallback mechanisms
- Validate correlation between keyboard generation and callback processing

**Integration Verification**:
- Database state changes after callback actions
- Event flow from callback to service to response
- Inline keyboard updates after actions
- User notification messages

Reference `internal/chatbot/keyboard_builder.go` for callback data formats and `internal/chatbot/command_processor.go` for callback handling logic.

### unit\migration_failure_test.go(NEW)

References: 

- internal\nudge\migrations.go
- internal\database\postgres.go

Create unit tests for database migration failure scenarios, addressing critical database operations gaps.

**Test Infrastructure**:
- Use testcontainers with PostgreSQL for real database testing
- Create helper functions to simulate migration failures
- Test both GORM migration failures and custom index creation failures

**Migration Failure Scenarios**:
1. **TestMigration_RunMigrationsFailure**: Test `RunMigrations` failure by using invalid table definitions or database permissions
2. **TestMigration_CreateIndexesFailure**: Test `createIndexes` failure by creating conflicting indexes or invalid SQL
3. **TestMigration_ValidateMigrationsFailure**: Test `ValidateMigrations` with missing tables/indexes to verify detection
4. **TestMigration_MigrateWithValidationFailure**: Test complete `MigrateWithValidation` flow with various failure points
5. **TestMigration_PartialMigrationRecovery**: Test recovery from partial migration state
6. **TestMigration_DatabaseConnectionFailure**: Test migration behavior when database becomes unavailable

**Error Handling Verification**:
- Verify proper error wrapping and error codes
- Check that failed migrations don't leave database in inconsistent state
- Validate error messages are descriptive and actionable
- Test rollback behavior where applicable

**Database State Testing**:
- Verify table and index existence after successful migrations
- Check that failed migrations don't create partial structures
- Test migration idempotency (running twice should be safe)

Reference `internal/nudge/migrations.go` for migration functions and `internal/database/postgres.go` for connection handling patterns.

### unit\llm_provider_error_test.go(NEW)

References: 

- internal\llm\service.go
- internal\llm\gemma_provider.go
- internal\llm\service_test.go

Create unit tests for LLM provider error scenarios, addressing LLM integration gaps identified in the analysis.

**Test Setup**:
- Create mock HTTP server to simulate Gemma API responses
- Use real LLM service with mocked provider for error injection
- Test both network-level and application-level errors

**LLM Error Scenarios**:
1. **TestLLMProvider_TimeoutError**: Simulate API timeout, verify ParseTask returns appropriate error and no TaskParsed event is published
2. **TestLLMProvider_HTTPErrorCodes**: Test 400, 401, 403, 429, 500, 502, 503 responses, verify error handling and retry logic
3. **TestLLMProvider_MalformedJSONResponse**: Return invalid JSON, verify parsing error and graceful degradation
4. **TestLLMProvider_MissingRequiredFields**: Return JSON missing task_description or due_date, verify validation failure
5. **TestLLMProvider_InvalidDateFormat**: Return invalid ISO 8601 dates, verify date parsing error handling
6. **TestLLMProvider_NetworkConnectionFailure**: Simulate network unavailability, verify connection error handling
7. **TestLLMProvider_LargeResponseHandling**: Test oversized responses, verify size limit handling

**Error Propagation Testing**:
- Verify errors are properly wrapped with context
- Check that LLM errors don't crash the service
- Validate that failed parsing doesn't publish TaskParsed events
- Test error message formatting for user feedback

**Integration with Event System**:
- Verify that LLM errors result in appropriate error events or user notifications
- Test that the chatbot service handles LLM failures gracefully
- Validate retry mechanisms and circuit breaker patterns if implemented

Reference `internal/llm/service.go` for LLM service logic and `internal/llm/gemma_provider.go` for provider implementation patterns.

### unit\scheduler_timing_test.go(NEW)

References: 

- internal\nudge\business_logic.go
- internal\nudge\service.go
- internal\scheduler\scheduler.go

Create unit tests for scheduler timing logic and business rules, addressing gaps in ReminderManager and TaskStatusManager testing.

**Test Infrastructure**:
- Implement clock abstraction interface to control time in tests
- Create test fixtures for tasks, reminders, and user settings
- Use enhanced mock repository for data persistence simulation

**ReminderManager Testing**:
1. **TestReminderManager_CalculateReminderTime**: Test reminder time calculation based on due date and user settings
2. **TestReminderManager_ShouldCreateNudge**: Test nudge creation logic based on overdue time, nudge count, and user preferences
3. **TestReminderManager_GetNextNudgeTime**: Test exponential backoff calculation for subsequent nudges
4. **TestReminderManager_NudgeIntervalLimits**: Test minimum and maximum nudge intervals
5. **TestReminderManager_UserSpecificSettings**: Test how user nudge settings affect timing calculations

**TaskStatusManager Testing**:
1. **TestTaskStatusManager_StatusTransitions**: Test valid and invalid status transitions (active→completed, active→snoozed, etc.)
2. **TestTaskStatusManager_SnoozeTask**: Test snooze functionality with due date updates
3. **TestTaskStatusManager_CompleteTask**: Test task completion with timestamp setting
4. **TestTaskStatusManager_ReactivateTask**: Test task reactivation from snoozed/completed states
5. **TestTaskStatusManager_TransitionValidation**: Test business rule validation for status changes

**Timing Edge Cases**:
- Test timezone handling (if implemented)
- Test daylight saving time transitions
- Test leap year and month boundary calculations
- Test very short and very long reminder intervals

**Business Rule Validation**:
- Test task priority effects on reminder timing
- Test maximum nudge count enforcement
- Test reminder scheduling for past due dates

Reference `internal/nudge/business_logic.go` for business logic implementations and `internal/nudge/service.go` for integration patterns.

### integration\event_bus_failure_test.go(NEW)

References: 

- internal\events\bus.go
- internal\events\types.go
- internal\chatbot\service.go
- internal\llm\service.go
- internal\nudge\service.go

Create integration tests for event bus failure scenarios and cross-service event communication, addressing event-driven architecture gaps.

**Test Setup**:
- Use real event bus implementation instead of mocks
- Create test scenarios that simulate event bus failures
- Set up multiple services to test cross-service communication

**Event Bus Failure Scenarios**:
1. **TestEventBus_SubscriptionFailure**: Test service behavior when event subscriptions fail during startup
2. **TestEventBus_PublishFailure**: Test service behavior when event publishing fails
3. **TestEventBus_EventDeliveryFailure**: Test handling of events that fail to deliver to subscribers
4. **TestEventBus_BusShutdownDuringOperation**: Test graceful handling when event bus shuts down during active operations
5. **TestEventBus_EventOrderingGuarantees**: Test event ordering and delivery guarantees under load
6. **TestEventBus_ConcurrentEventHandling**: Test concurrent event processing and potential race conditions

**Cross-Service Event Flow Testing**:
1. **TestCrossService_MessageToTaskFlow**: Test complete flow from chatbot→LLM→nudge with real event bus
2. **TestCrossService_TaskActionFlow**: Test task action events flowing between chatbot and nudge services
3. **TestCrossService_ReminderFlow**: Test reminder events from scheduler to chatbot
4. **TestCrossService_EventPayloadValidation**: Test event payload validation across service boundaries
5. **TestCrossService_ErrorEventPropagation**: Test how error events propagate between services

**Event Payload and Timing Testing**:
- Verify event payload integrity across service boundaries
- Test event correlation ID tracking through complete flows
- Validate event timestamps and ordering
- Test event serialization/deserialization

**Error Recovery Testing**:
- Test service recovery after event bus failures
- Verify event replay mechanisms if implemented
- Test dead letter queue handling if present

Reference `internal/events/bus.go` for event bus implementation and existing service event handling patterns from `internal/chatbot/service.go`, `internal/llm/service.go`, and `internal/nudge/service.go`.

### docs\test_strategy.md(NEW)

References: 

- docs\mvp_prd.md
- Makefile

Create comprehensive test strategy documentation that explains the testing approach and rationale for the NudgeBot MVP.

**Document Structure**:

**1. Testing Philosophy**
- Explain the three-tier testing approach: Unit → Integration → End-to-End
- Describe the focus on MVP requirements and PRD traceability
- Outline the balance between real services and mocks/stubs

**2. Test Categories and Scope**
- **Unit Tests**: Service logic, business rules, error handling, validation
- **Integration Tests**: Cross-service flows, database operations, HTTP endpoints
- **End-to-End Tests**: Complete user journeys from webhook to database

**3. Testing Infrastructure**
- **Real Services**: PostgreSQL (via testcontainers), Event Bus, HTTP handlers
- **Stub Services**: LLM provider, Telegram provider (to avoid external dependencies)
- **Mock Services**: Repositories, event buses (for unit tests)
- **Test Utilities**: Clock abstraction, webhook payload generators, assertion helpers

**4. Critical Flow Coverage**
- Map each MVP user story to specific test scenarios
- Explain priority ranking: scheduler flows (highest), command processing, error scenarios
- Document coverage for each of the five critical areas

**5. Test Execution Strategy**
- Explain build tag usage (`//go:build integration`)
- Document make targets: `test-unit`, `test-integration`, `test-all`
- Describe CI/CD integration and parallel execution considerations

**6. Error Scenario Testing**
- Database failures and migration issues
- External service timeouts and errors
- Event bus failures and recovery
- Concurrent access and race conditions

**7. Maintenance and Evolution**
- Guidelines for adding new tests as features evolve
- Patterns for maintaining test reliability
- Strategies for test performance and execution time

Reference the test coverage matrix and explain how it ensures PRD requirement traceability.

### internal\common\clock.go(NEW)

References: 

- internal\scheduler\scheduler.go
- internal\nudge\business_logic.go

Create clock abstraction interface to enable deterministic timing tests for scheduler and reminder functionality.

**Interface Definition**:
- Define `Clock` interface with methods: `Now() time.Time`, `After(duration) <-chan time.Time`, `Sleep(duration)`
- Implement `RealClock` struct that wraps standard time functions
- Implement `MockClock` struct for testing with controllable time

**MockClock Features**:
- Allow setting current time for deterministic tests
- Provide `Advance(duration)` method to move time forward
- Support multiple time zones if needed
- Enable fast-forwarding through long durations instantly

**Integration Points**:
- Modify scheduler to accept Clock interface as dependency
- Update ReminderManager to use Clock for time calculations
- Ensure all time-dependent business logic uses Clock abstraction

**Usage Patterns**:
- Unit tests use MockClock for instant, deterministic timing
- Integration tests can use MockClock or RealClock based on scenario
- Production code uses RealClock injected at startup

This abstraction is essential for testing scheduler ticks, reminder timing, and nudge intervals without actual time delays in tests.